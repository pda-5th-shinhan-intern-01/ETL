from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
import time
import redis
import json
import os
from dotenv import load_dotenv
from datetime import datetime

# .env Î∂àÎü¨Ïò§Í∏∞
load_dotenv()
REDIS_HOST = os.getenv("REDIS_HOST", "localhost")
REDIS_PORT = int(os.getenv("REDIS_PORT", 6379))
REDIS_PASSWORD = os.getenv("REDIS_PASSWORD", None)

r = redis.Redis(
    host=REDIS_HOST,
    port=REDIS_PORT,
    password=REDIS_PASSWORD if REDIS_PASSWORD else None,
    decode_responses=True
)

# ÏßÄÌëú Îß§Ìïë
INDICATOR_MAP = {
    "Core Inflation Rate YoY": "Core CPI",
    "Core PPI YoY": "Core PPI",
    "Core PCE YoY": "Core PCE",
    "GDP Growth Rate QoQ 2nd Est": "GDP",
    "Core Retail Sales": "Retail Sales",
    "Industrial Production": "Industrial Production",
    "Unemployment Rate": "Unemployment Rate",
    "Nonfarm Payrolls": "Nonfarm Payrolls",
    "ISM Manufacturing PMI": "ISM Manufacturing PMI",
    "ISM Non-Manufacuring PMI": "ISM Services PMI",
    "Michigan Consumer Sentiment Final": "UM Consumer Sentiment",
    "CB Consumer Confidence": "CB Consumer Confidence",
    "Initial Jobless Claims": "Initial Jobless Claims",
    "Durable Goods Orders": "Durable Goods Orders",
    "New Home Sales": "New Home Sales",
    "Existing Home Sales": "Existing Home Sales"
}

INDICATOR_KR_MAP = {
    "Core CPI": "Í∑ºÏõê ÏÜåÎπÑÏûêÎ¨ºÍ∞ÄÏßÄÏàò",
    "Core PPI": "Í∑ºÏõê ÏÉùÏÇ∞ÏûêÎ¨ºÍ∞ÄÏßÄÏàò",
    "Core PCE": "Í∑ºÏõê Í∞úÏù∏ÏÜåÎπÑÏßÄÏ∂úÎ¨ºÍ∞ÄÏßÄÏàò",
    "GDP": "Íµ≠ÎÇ¥Ï¥ùÏÉùÏÇ∞ Î∂ÑÍ∏∞ ÏÑ±Ïû•Î•†",
    "Retail Sales": "Í∑ºÏõê ÏÜåÎß§ÌåêÎß§",
    "Industrial Production": "ÏÇ∞ÏóÖÏÉùÏÇ∞",
    "Unemployment Rate": "Ïã§ÏóÖÎ•†",
    "Nonfarm Payrolls": "ÎπÑÎÜçÏóÖÎ∂ÄÎ¨∏ Ïã†Í∑ú Í≥†Ïö©Ïûê Ïàò",
    "ISM Manufacturing PMI": "ISM Ï†úÏ°∞ÏóÖ PMI",
    "ISM Services PMI": "ISM ÏÑúÎπÑÏä§ÏóÖ PMI",
    "UM Consumer Sentiment": "ÎØ∏ÏãúÍ∞ÑÎåÄ ÏÜåÎπÑÏûêÏã¨Î¶¨ÏßÄÏàò",
    "CB Consumer Confidence": "Ïª®ÌçºÎü∞Ïä§Î≥¥Îìú ÏÜåÎπÑÏûêÏã†Î¢∞ÏßÄÏàò",
    "Initial Jobless Claims": "Ïã†Í∑ú Ïã§ÏóÖÏàòÎãπ Ï≤≠Íµ¨Í±¥Ïàò",
    "Durable Goods Orders": "ÎÇ¥Íµ¨Ïû¨ Ï£ºÎ¨∏",
    "New Home Sales": "Ïã†Í∑ú Ï£ºÌÉù ÌåêÎß§Í±¥Ïàò",
    "Existing Home Sales": "Í∏∞Ï°¥ Ï£ºÌÉù ÌåêÎß§Í±¥Ïàò"
}


def crawl_tradingeconomics():
    options = Options()
    options.add_argument("--disable-gpu")
    options.add_argument("user-agent=Mozilla/5.0")

    driver = webdriver.Chrome(options=options)
    driver.get("https://tradingeconomics.com/united-states/calendar")
    time.sleep(5)

    soup = BeautifulSoup(driver.page_source, "html.parser")
    rows = soup.select("table#calendar > tbody > tr")
    print(f"Ï¥ù row Ïàò: {len(rows)}")

    result = []

    for row in rows:
        cells = row.find_all("td")
        if len(cells) < 9:
            continue

        date_td = cells[0]
        date_classes = date_td.get("class", [])
        current_date = next((cls for cls in date_classes if cls.count("-") == 2), None)
        if not current_date:
            continue

        time_text = cells[0].text.strip()
        full_date = f"{current_date} {time_text}"

        event_tag = cells[4].select_one("a")
        if not event_tag:
            continue
        raw_event = event_tag.text.strip()

        if raw_event not in INDICATOR_MAP:
            continue

        event = INDICATOR_MAP[raw_event]
        event_kor = INDICATOR_KR_MAP.get(event, event)

        actual_tag = cells[5].select_one("span")
        actual = actual_tag.text.strip() if actual_tag else ""

        previous_tag = cells[6].select_one("span")
        previous = previous_tag.text.strip() if previous_tag else ""

        forecast_tag = cells[8].select_one("a, span")
        forecast = forecast_tag.text.strip() if forecast_tag else ""

        result.append({
            "event": event,
            "event_kor": event_kor,
            "date": full_date,
            "actual": actual,
            "forecast": forecast,
            "previous": previous
        })

    driver.quit()
    return result


def save_to_redis(data_list):
    for item in data_list:
        try:
            dt = datetime.strptime(item['date'], "%Y-%m-%d %I:%M %p")
        except Exception as e:
            print(f"‚ùå ÎÇ†Ïßú ÌååÏã± Ïã§Ìå®: {item['date']} ({e})")
            continue

        date_part = dt.strftime("%Y-%m-%d")
        time_part = dt.strftime("%H:%M")

        event_code = item["event"].upper().replace(" ", "_")  # Redis-safe key
        redis_key = f"event:{date_part}:{time_part}:{event_code}"

        r.set(redis_key, json.dumps(item))
        print(f"‚úÖ Ï†ÄÏû•Îê®: {redis_key}")


if __name__ == "__main__":
    data = crawl_tradingeconomics()
    for item in data:
        print(f"{item['event']} - üìÖ {item['date']} | Ïã§Ï†ú: {item['actual']} | ÏòàÏ∏°: {item['forecast']} | Ï†ÑÏõî: {item['previous']}")
    save_to_redis(data)
