from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
import time
import redis
from pathlib import Path
import json
import os
from dotenv import load_dotenv
from datetime import datetime
import pymysql

# .env Î∂àÎü¨Ïò§Í∏∞
base_dir = Path(__file__).resolve().parent.parent
load_dotenv(dotenv_path=base_dir / ".env")

REDIS_HOST = os.getenv("REDIS_HOST", "localhost")
REDIS_PORT = int(os.getenv("REDIS_PORT", 6379))
REDIS_PASSWORD = os.getenv("REDIS_PASSWORD", None)

r = redis.Redis(
    host=REDIS_HOST,
    port=REDIS_PORT,
    password=REDIS_PASSWORD if REDIS_PASSWORD else None,
    decode_responses=True
)

# ÏßÄÌëú Îß§Ìïë
INDICATOR_MAP = {
    "Core Inflation Rate YoY": "Core CPI",
    "Core PPI YoY": "Core PPI",
    "Core PCE YoY": "Core PCE",
    "GDP Growth Rate QoQ 2nd Est": "GDP",
    "Core Retail Sales": "Retail Sales",
    "Industrial Production": "Industrial Production",
    "Unemployment Rate": "Unemployment Rate",
    "Nonfarm Payrolls": "Nonfarm Payrolls",
    "ISM Manufacturing PMI": "ISM Manufacturing PMI",
    "ISM Non-Manufacuring PMI": "ISM Services PMI",
    "Michigan Consumer Sentiment Final": "UM Consumer Sentiment",
    "CB Consumer Confidence": "CB Consumer Confidence",
    "Initial Jobless Claims": "Initial Jobless Claims",
    "Durable Goods Orders": "Durable Goods Orders",
    "New Home Sales": "New Home Sales",
    "Existing Home Sales": "Existing Home Sales"
}

INDICATOR_KR_MAP = {
    "Core CPI": "Í∑ºÏõê ÏÜåÎπÑÏûêÎ¨ºÍ∞ÄÏßÄÏàò",
    "Core PPI": "Í∑ºÏõê ÏÉùÏÇ∞ÏûêÎ¨ºÍ∞ÄÏßÄÏàò",
    "Core PCE": "Í∑ºÏõê Í∞úÏù∏ÏÜåÎπÑÏßÄÏ∂úÎ¨ºÍ∞ÄÏßÄÏàò",
    "GDP": "Íµ≠ÎÇ¥Ï¥ùÏÉùÏÇ∞ Î∂ÑÍ∏∞ ÏÑ±Ïû•Î•†",
    "Retail Sales": "Í∑ºÏõê ÏÜåÎß§ÌåêÎß§",
    "Industrial Production": "ÏÇ∞ÏóÖÏÉùÏÇ∞",
    "Unemployment Rate": "Ïã§ÏóÖÎ•†",
    "Nonfarm Payrolls": "ÎπÑÎÜçÏóÖÎ∂ÄÎ¨∏ Ïã†Í∑ú Í≥†Ïö©Ïûê Ïàò",
    "ISM Manufacturing PMI": "ISM Ï†úÏ°∞ÏóÖ PMI",
    "ISM Services PMI": "ISM ÏÑúÎπÑÏä§ÏóÖ PMI",
    "UM Consumer Sentiment": "ÎØ∏ÏãúÍ∞ÑÎåÄ ÏÜåÎπÑÏûêÏã¨Î¶¨ÏßÄÏàò",
    "CB Consumer Confidence": "Ïª®ÌçºÎü∞Ïä§Î≥¥Îìú ÏÜåÎπÑÏûêÏã†Î¢∞ÏßÄÏàò",
    "Initial Jobless Claims": "Ïã†Í∑ú Ïã§ÏóÖÏàòÎãπ Ï≤≠Íµ¨Í±¥Ïàò",
    "Durable Goods Orders": "ÎÇ¥Íµ¨Ïû¨ Ï£ºÎ¨∏",
    "New Home Sales": "Ïã†Í∑ú Ï£ºÌÉù ÌåêÎß§Í±¥Ïàò",
    "Existing Home Sales": "Í∏∞Ï°¥ Ï£ºÌÉù ÌåêÎß§Í±¥Ïàò"
}


def crawl_tradingeconomics():
    options = Options()
    options.add_argument("--disable-gpu")
    options.add_argument("user-agent=Mozilla/5.0")

    driver = webdriver.Chrome(options=options)
    driver.get("https://tradingeconomics.com/united-states/calendar")
    time.sleep(5)

    soup = BeautifulSoup(driver.page_source, "html.parser")
    rows = soup.select("table#calendar > tbody > tr")
    print(f"Ï¥ù row Ïàò: {len(rows)}")

    result = []

    for row in rows:
        cells = row.find_all("td")
        if len(cells) < 9:
            continue

        date_td = cells[0]
        date_classes = date_td.get("class", [])
        current_date = next((cls for cls in date_classes if cls.count("-") == 2), None)
        if not current_date:
            continue

        time_text = cells[0].text.strip()
        full_date = f"{current_date} {time_text}"

        event_tag = cells[4].select_one("a")
        if not event_tag:
            continue
        raw_event = event_tag.text.strip()

        if raw_event not in INDICATOR_MAP:
            continue

        event = INDICATOR_MAP[raw_event]
        event_kor = INDICATOR_KR_MAP.get(event, event)

        actual_tag = cells[5].select_one("span")
        actual = actual_tag.text.strip() if actual_tag else ""

        previous_tag = cells[6].select_one("span")
        previous = previous_tag.text.strip() if previous_tag else ""

        forecast_tag = cells[8].select_one("a, span")
        forecast = forecast_tag.text.strip() if forecast_tag else ""

        result.append({
            "event": event,
            "event_kor": event_kor,
            "date": full_date,
            "actual": actual,
            "forecast": forecast,
            "previous": previous
        })

    driver.quit()
    return result


# def save_to_redis(data_list):
#     for item in data_list:
#         try:
#             dt = datetime.strptime(item['date'], "%Y-%m-%d %I:%M %p")
#         except Exception as e:
#             print(f"‚ùå ÎÇ†Ïßú ÌååÏã± Ïã§Ìå®: {item['date']} ({e})")
#             continue
#
#         date_part = dt.strftime("%Y-%m-%d")
#         time_part = dt.strftime("%H:%M")
#
#         event_code = item["event"].upper().replace(" ", "_")  # Redis-safe key
#         redis_key = f"event:{date_part}:{time_part}:{event_code}"
#
#         r.set(redis_key, json.dumps(item))
#         print(f"‚úÖ Ï†ÄÏû•Îê®: {redis_key}")
#

DB_PORT = int(os.getenv("DB_PORT", 3306))
DB_NAME = os.getenv("DB_NAME")

# env = os.getenv("ENV", "dev")
env_file = base_dir / f".env-dev"
if os.path.exists(env_file):
    load_dotenv(env_file, override=True)
DB_USER = os.getenv("DB_USER")
DB_HOST = os.getenv("DB_HOST","127.0.0.1")
DB_PASSWORD = os.getenv("DB_PASSWORD")
print(DB_USER, DB_HOST, DB_PASSWORD)

def save_to_db(data_list):
    conn = pymysql.connect(
        host=DB_HOST,
        port=DB_PORT,
        user=DB_USER,
        password=DB_PASSWORD,
        db=DB_NAME,
        charset='utf8mb4'
    )
    cursor = conn.cursor()

    for item in data_list:
        try:
            dt = datetime.strptime(item['date'], "%Y-%m-%d %I:%M %p")
        except Exception as e:
            print(f"‚ùå ÎÇ†Ïßú ÌååÏã± Ïã§Ìå®: {item['date']} ({e})")
            continue

        name = item['event']
        event = item['event_kor']
        date = dt.date()
        previous = item['previous']
        forecast = item['forecast']
        actual = item['actual']

        sql = """
                INSERT INTO economicevent (name, event,date, previous, forecast, actual)
                VALUES (%s, %s, %s, %s, %s)
            """
        values = (name, event,date, previous, forecast, actual)

        cursor.execute(sql, values)
        print(f"‚úÖ Ï†ÄÏû• ÏôÑÎ£å: {name} ({date})")

    conn.commit()
    cursor.close()
    conn.close()

if __name__ == "__main__":

    data = crawl_tradingeconomics()
    for item in data:
        print(f"{item['event']} - üìÖ {item['date']} | Ïã§Ï†ú: {item['actual']} | ÏòàÏ∏°: {item['forecast']} | Ï†ÑÏõî: {item['previous']}")
    # save_to_redis(data)
    save_to_db(data)