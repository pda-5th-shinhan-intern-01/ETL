import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import re


"""
1. FOMC íšŒì˜ë¡ ë¦¬ìŠ¤íŠ¸ í¬ë¡¤ë§
2. urlë¡œ FOMC íšŒì˜ë¡ ìƒì„¸ í¬ë¡¤ë§
"""

BASE_URL = "https://www.federalreserve.gov"

# FOMC íšŒì˜ë¡ ë¦¬ìŠ¤íŠ¸ í¬ë¡¤ë§
def crawl_fomc_statement_list() -> list:
    url = "https://www.federalreserve.gov/monetarypolicy/fomccalendars.htm"
    response = requests.get(url)
    response.raise_for_status()

    soup = BeautifulSoup(response.text, "html.parser")
    
    # ğŸ” monetary ì„±ëª…ì„œ ê´€ë ¨ ë§í¬ë§Œ ì¶”ì¶œ
    links = soup.select('a[href*="/newsevents/pressreleases/monetary"]')

    statements = []
    for link in links:
        href = link.get('href')

        # a1.htmì€ ì œì™¸ â†’ ì„±ëª…ì„œë§Œ ì¶”ì¶œ
        if not href.endswith("a.htm"):
            continue

        # ë‚ ì§œ ì¶”ì¶œ
        date = extract_date_from_url(href)
        if not date:
            continue
        
        # ì˜ìƒ ë§í¬ ì¶”ì¶œ
        video_url = get_press_conference_video_url(date)

        full_url = urljoin(BASE_URL, href)
        statements.append({
            "event": "fomc",
            "date": date,
            "url": full_url,
            "video_url": video_url
        })

    return statements


# fomc ê°œë³„ íšŒì˜ë¡ ìƒì„¸ ë‚´ìš© í¬ë¡¤ë§
def crawl_fomc_statement_text(url: str) -> str:
    try:
        response = requests.get(url)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, 'html.parser')

        # FOMC ë³¸ë¬¸ì´ ë“¤ì–´ ìˆëŠ” div ì„ íƒ
        content_div = soup.find('div', class_='col-xs-12 col-sm-8 col-md-8')

        if not content_div:
            return "ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        # p íƒœê·¸ ê¸°ì¤€ìœ¼ë¡œ í…ìŠ¤íŠ¸ ëª¨ìŒ
        paragraphs = content_div.find_all('p')
        full_text = "\n\n".join(p.get_text(strip=True) for p in paragraphs)

        return full_text

    except requests.exceptions.RequestException as e:
        return f"ìš”ì²­ ì‹¤íŒ¨: {e}"


# ë‚ ì§œ ì¶”ì¶œ
def extract_date_from_url(href: str) -> str:
    # URLì—ì„œ ë‚ ì§œ ì¶”ì¶œ (ì˜ˆ: monetary20250129a.htm â†’ 2025-01-29)
    import re
    match = re.search(r"monetary(\d{4})(\d{2})(\d{2})", href)
    if match:
        year, month, day = match.groups()
        return f"{year}-{month}-{day}"
    return None


# FOMC íšŒì˜ë¡ video url ì¶”ì¶œ
def get_press_conference_video_url(meeting_date: str) -> str:
    calendar_url = f"{BASE_URL}/monetarypolicy/fomccalendars.htm"
    response = requests.get(calendar_url)
    response.raise_for_status()
    soup = BeautifulSoup(response.text, "html.parser")

    # ë‚ ì§œ í¬ë§·: 2025-01-29 â†’ 20250129
    date_str = meeting_date.replace("-", "")
    
    # 'Press Conference' ë§í¬ ì¤‘ ë‚ ì§œ í¬í•¨ëœ href ì°¾ê¸°
    link_tag = soup.find("a", href=re.compile(f"fomcpresconf{date_str}"), string=re.compile("Press Conference", re.I))
    
    if link_tag:
        return urljoin(BASE_URL, link_tag['href'])

    return ""

def main():
    # fomc_text = crawl_fomc_statement_text('https://www.federalreserve.gov/newsevents/pressreleases/monetary20250129a.htm')
    # print(fomc_text)
    statement_list = crawl_fomc_statement_list()
    for item in statement_list:
        print(item)
    
if __name__ == "__main__":
    main()